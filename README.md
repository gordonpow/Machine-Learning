
# 1. SVM 的訓練過程推導

支持向量機（SVM）的目標是找到一個超平面將資料點進行分類，並且使得分類邊界與資料點之間的距離最大化。

## a. 定義問題

假設我們有一組標註好的資料集 \(\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}\)，其中 \(x_i \in \mathbb{R}^d\) 是資料點，\(y_i \in \{-1, +1\}\) 是其標籤。

SVM 的目標是找到一個超平面，該超平面可以由以下方程表示：

\[
w \cdot x + b = 0
\]

\frac{\partial x}{\partial y} 

其中，\(w \in \mathbb{R}^d\) 是超平面的法向量，\(b\) 是偏置項。

## b. 最大化間隔

SVM 的核心目標是最大化從支持向量（離超平面最近的資料點）到超平面的距離。假設超平面上的資料點的距離是 1，那麼兩個類別的資料點應該分開並且距離最大的邊界為 2。

我們的目標是最大化此間隔，即最大化 \(\frac{2}{||w||}\)。

## c. 約束條件

為了使所有資料點被正確分類，對於每個資料點 \((x_i, y_i)\) 必須滿足：

\[
y_i (w \cdot x_i + b) \geq 1
\]

這樣可以保證資料點位於邊界兩側，並且間隔最大化。

## d. 目標函數

SVM 的優化問題可以表述為：

\[
\min_{w, b} \frac{1}{2} ||w||^2
\]

同時，對所有的資料點，必須滿足約束條件：

\[
y_i (w \cdot x_i + b) \geq 1, \quad i = 1, 2, \dots, n
\]

## e. 拉格朗日乘數法

為了解這個問題，我們使用拉格朗日乘數法，將約束條件引入到目標函數中。拉格朗日函數為：

\[
L(w, b, \alpha) = \frac{1}{2} ||w||^2 - \sum_{i=1}^{n} \alpha_i \left( y_i (w \cdot x_i + b) - 1 \right)
\]

其中，\(\alpha_i \geq 0\) 是拉格朗日乘數。

## f. KKT條件與最優解

經過拉格朗日對偶轉換和 KKT 條件（Karush-Kuhn-Tucker 條件），可以得到最終的優化問題，解出 \(w\) 和 \(b\)。

---

# 2. MLP 的訓練過程推導

多層感知機（MLP）是一個前向神經網絡，包含至少一層隱藏層，並使用反向傳播算法訓練模型。

## a. 前向傳播

假設一個具有兩層的 MLP，每一層包含若干個神經元。對於第 \(l\) 層的輸入 \(x_l\) 和權重 \(W_l\)，該層的輸出 \(y_l\) 由以下公式給出：

\[
y_l = f(W_l x_l + b_l)
\]

其中，\(f\) 是激活函數，\(W_l\) 是該層的權重，\(b_l\) 是偏置。

## b. 損失函數

MLP 的目標是最小化損失函數，通常使用交叉熵或均方誤差損失函數。對於回歸問題，使用均方誤差（MSE）：

\[
L = \frac{1}{2} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\]

其中，\(y_i\) 是真實值，\(\hat{y}_i\) 是預測值。

## c. 反向傳播

反向傳播算法用於更新網絡的權重。首先計算損失對每一層的偏導數，然後利用鏈式法則來計算每層的梯度。

對於每一層，權重的更新公式為：

\[
W_l^{new} = W_l^{old} - \eta \frac{\partial L}{\partial W_l}
\]

其中，\(\eta\) 是學習率，\(\frac{\partial L}{\partial W_l}\) 是該層權重的梯度。

## d. 梯度下降更新規則

權重更新公式：

\[
w^{*} = w + \Delta w
\]

其中，\(\Delta w = - \eta \nabla_w L\)，即梯度下降中每次步長的更新。

---
